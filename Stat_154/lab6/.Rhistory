install.packages(glmnet)
install.packages("glmnet")
library(glmnet)
x=model.matrix(Salary∼.,Hitters)[,-1]
x=model.matrix(Salary~.,Hitters)[,-1]
data(Hitters)
??Hitters
library(ISLR)
install.packages("ISLR")
library(ISLR)
data(Hitters)
Hitters=na.omit(Hitters)
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
data(Hitters)
Hitters=na.omit(Hitters)
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
ridge.mod$beta
ridge.mod$lambda
ridge.mod$lambda[50]
ceof(ridge.mod)[,50]
coef(ridge.mod)[,50]
predict(ridge.mod,s=50,type="coefficients")[1:20,]
set . seed (1)
train=sample(1:nrow(x), nrow(x)/2)
set.seed (1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh =1e -12)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh =1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
# A very large lambda won't help
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)
mean((ridge.pred-y.test)^2)
lm(y∼x, subset=train)
ridge.pred = predict(ridge.mod,s=0,newx=x[test,],exact=T)
# Setseed for replication purpose
set.seed (1)
# Split into training set and test set
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
# A very large lambda won't help
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred = predict(ridge.mod,s=0,newx=x[test,],exact=T)
ridge.pred = predict(ridge.mod,s=0,newx=x[test,])
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients")[1:20,]
predict(ridge.mod,s=0,type="coefficients")[1:20,]
set.seed(1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda .min
bestlam=cv.out$lambda.min
bestlam
plot(cv.out)
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed (1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda .min
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
plot(cv.out)
install.packages("pls")
library(pls)
set.seed (2)
pcr.fit=pcr(Salary~., data=Hitters ,scale=TRUE,
validation ="CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP")
set.seed (1)
pcr.fit=pcr(Salary∼., data=Hitters,subset=train,scale=TRUE,
pcr.fit=pcr(Salary~., data=Hitters,subset=train,scale=TRUE,
validation ="CV")
validationplot(pcr.fit,val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
ridge.pred = predict(ridge.mod,s=0,newx=x[test,], newy = y[test,], exact = T)
ridge.pred = predict(ridge.mod,s=0,newx=x[test,], newy = y[test], exact = T)
pcr.fit=pcr(y∼x,scale=TRUE,ncomp=7)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
# PLSR
pls.fit=plsr(Salary∼., data=Hitters ,subset=train,scale=TRUE, validation = "CV")
# PLSR
pls.fit=plsr(Salary~., data=Hitters ,subset=train,scale=TRUE, validation = "CV")
# PLSR
set . seed (1)
# PLSR
set.seed (1)
pls.fit=plsr(Salary~., data=Hitters ,subset=train,scale=TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
pls.fit=plsr(Salary~., data=Hitters ,scale=TRUE,ncomp=2)
# Finally, use pls on the full data to get the regression estimates
pls.fit=plsr(Salary~., data=Hitters ,scale=TRUE,ncomp=2)
summary(pls.fit)
