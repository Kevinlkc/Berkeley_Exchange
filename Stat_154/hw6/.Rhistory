geom_line(aes(x = 1:10, y = plotData$sulphates, color = 'sulphates')) +
geom_line(aes(x = 1:10, y = plotData$alcohol, color = 'alcohol')) +
labs(
x = "Number of Dimensions",
y = "Coefficients",
title = "The Comparison of the Coefficients w.r.t. # of Dimensions Kept in the Model"
)
# The more Dimensions Kept in the Model of PLSR, the more unstable the coefficients are. There's an expanding effect. (The same logic follows for the more flexible a model is, the more variance it will generate. And also for PCR.) The coefficients could blow up in some circumstances, that's even more severe in PLSR than PCR. Note that the coefficients when # = 10 is identical to the coefficients of the OLS regression.
# RR
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = seq(0, 0.45, 0.05))
plotData <- data.frame(t(as.matrix(RR$beta)))
ggplot() + theme_bw() +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$fixed.acidity, color = 'fixedAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$volatile.acidity, color = 'volAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$citric.acid, color = 'citricAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$residual.sugar, color = 'resSugar')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$chlorides, color = 'chlorides')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$free.sulfur.dioxide, color = 'freeSO2')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$total.sulfur.dioxide, color = 'totalSO2')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$density, color = 'density')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$sulphates, color = 'sulphates')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$alcohol, color = 'alcohol')) +
labs(
x = "Lambda",
y = "Coefficients",
title = "The Comparison of the Coefficients w.r.t. Lambda in Rigde"
)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 1, lambda = c(0, 0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.25))
plotData <- data.frame(t(as.matrix(RR$beta)))
ggplot() + theme_bw() +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$fixed.acidity, color = 'fixedAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$volatile.acidity, color = 'volAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$citric.acid, color = 'citricAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$residual.sugar, color = 'resSugar')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$chlorides, color = 'chlorides')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$free.sulfur.dioxide, color = 'freeSO2')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$total.sulfur.dioxide, color = 'totalSO2')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$density, color = 'density')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$sulphates, color = 'sulphates')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$alcohol, color = 'alcohol')) +
labs(
x = "Lambda",
y = "Coefficients",
title = "The Comparison of the Coefficients w.r.t. Lambda in Lasso"
)
# CV
# Create 5 folds
trainingSet <- trainingSet %>%
mutate(instant = 1:nrow(trainingSet), fold = 0)
tempdata <- trainingSet
for (i in 1:5){
num = 780
if (i == 5){num = 778}
temp <-  sample_n(tempdata, num)
tempdata <- tempdata %>%
filter(!(instant %in% temp$instant))
trainingSet[trainingSet$instant %in% temp$instant,] <- trainingSet %>%
filter(instant %in% temp$instant) %>%
mutate(fold = i)
}
# For each model, for each fold, calculate the best tuning parameter
# First Consider PCR
trainingSet <- trainingSet %>% select(-instant)
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
for (j in 1:10){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
PCR <- pcr(pH ~ .-fold, data = trainingSet2, scale = T)
pred <- test %>% pull(-pH) %*% t(PCR$coefficients[,,j])
MSE[i,j] = mean((pred - test$pH)^2)
}
}
MSE
which.min(apply(MSE,2,mean))
# Find the lowest MSE
# Then Consider PLSR
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
for (j in 1:10){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
PLSR <- plsr(pH ~ .-fold, data = trainingSet2, scale = T)
pred <- test %>% pull(-pH) %*% t(PLSR$coefficients[,,j])
MSE[i,j] = mean((pred - test$pH)^2)
}
}
MSE
which.min(apply(MSE,2,mean))
# Find the lowest MSE
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,0.45,0.05)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,0.45,0.05)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 1, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(pls)
library(glmnet)
library(stargazer)
# Data Input
set.seed(1)
white <- read.csv("winequality-white.csv", sep = ';')
red <- read.csv("winequality-red.csv", sep = ';')
data <- bind_rows(white, red)
# Toy Model
basicModel <- lm(pH ~ ., data = data %>% select(-quality))
stargazer(basicModel)
# Threeway Hold-out Method
# Standardize our data from the very beginning
set.seed(1)
data <- data.frame(scale(data))
data <- data %>%
mutate(index = 1:nrow(data)) %>%
select(-quality)
trainingSet <- sample_frac(data, 0.6, replace = FALSE)
validationSet <- sample_frac(data %>% filter(!(index %in% trainingSet$index)), 0.5, replace = F)
testSet <- data %>% filter(!(index %in% trainingSet$index) & !(index %in% validationSet$index))
trainingSet <- trainingSet %>% select(-index)
validationSet <- validationSet %>% select(-index)
testSet <- testSet %>% select(-index)
# 1. Baseline OLS Model
OLS <- lm(pH ~., data = trainingSet)
OLS$coefficients
# 2. PCR
PCR <- pcr(pH ~., data = trainingSet, scale = T)
plotData <- data.frame(t(PCR$coefficients[,1,]))
ggplot() + theme_bw() +
geom_line(aes(x = 1:10, y = plotData$fixed.acidity, color = 'fixedAcid')) +
geom_line(aes(x = 1:10, y = plotData$volatile.acidity, color = 'volAcid')) +
geom_line(aes(x = 1:10, y = plotData$citric.acid, color = 'citricAcid')) +
geom_line(aes(x = 1:10, y = plotData$residual.sugar, color = 'resSugar')) +
geom_line(aes(x = 1:10, y = plotData$chlorides, color = 'chlorides')) +
geom_line(aes(x = 1:10, y = plotData$free.sulfur.dioxide, color = 'freeSO2')) +
geom_line(aes(x = 1:10, y = plotData$total.sulfur.dioxide, color = 'totalSO2')) +
geom_line(aes(x = 1:10, y = plotData$density, color = 'density')) +
geom_line(aes(x = 1:10, y = plotData$sulphates, color = 'sulphates')) +
geom_line(aes(x = 1:10, y = plotData$alcohol, color = 'alcohol')) +
labs(
x = "Number of PCs",
y = "Coefficients",
title = "The Comparison of the Coefficients w.r.t. # of PCs Kept in the Model"
)
# The more Principal Components Kept in the Model, the more unstable the coefficients are. There's an expanding effect. (The same logic follows for the more flexible a model is, the more variance it will generate.) The coefficients could blow up in some circumstances, possibly due to multicolinearity. Note that the coefficients when # = 10 is identical to the coefficients of the OLS regression.
# 2. PLSR
PLSR <- plsr(pH ~., data = trainingSet, scale = T)
plotData <- data.frame(t(PLSR$coefficients[,1,]))
ggplot() + theme_bw() +
geom_line(aes(x = 1:10, y = plotData$fixed.acidity, color = 'fixedAcid')) +
geom_line(aes(x = 1:10, y = plotData$volatile.acidity, color = 'volAcid')) +
geom_line(aes(x = 1:10, y = plotData$citric.acid, color = 'citricAcid')) +
geom_line(aes(x = 1:10, y = plotData$residual.sugar, color = 'resSugar')) +
geom_line(aes(x = 1:10, y = plotData$chlorides, color = 'chlorides')) +
geom_line(aes(x = 1:10, y = plotData$free.sulfur.dioxide, color = 'freeSO2')) +
geom_line(aes(x = 1:10, y = plotData$total.sulfur.dioxide, color = 'totalSO2')) +
geom_line(aes(x = 1:10, y = plotData$density, color = 'density')) +
geom_line(aes(x = 1:10, y = plotData$sulphates, color = 'sulphates')) +
geom_line(aes(x = 1:10, y = plotData$alcohol, color = 'alcohol')) +
labs(
x = "Number of Dimensions",
y = "Coefficients",
title = "The Comparison of the Coefficients w.r.t. # of Dimensions Kept in the Model"
)
# The more Dimensions Kept in the Model of PLSR, the more unstable the coefficients are. There's an expanding effect. (The same logic follows for the more flexible a model is, the more variance it will generate. And also for PCR.) The coefficients could blow up in some circumstances, that's even more severe in PLSR than PCR. Note that the coefficients when # = 10 is identical to the coefficients of the OLS regression.
# RR
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = seq(0, 0.45, 0.05))
plotData <- data.frame(t(as.matrix(RR$beta)))
ggplot() + theme_bw() +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$fixed.acidity, color = 'fixedAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$volatile.acidity, color = 'volAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$citric.acid, color = 'citricAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$residual.sugar, color = 'resSugar')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$chlorides, color = 'chlorides')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$free.sulfur.dioxide, color = 'freeSO2')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$total.sulfur.dioxide, color = 'totalSO2')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$density, color = 'density')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$sulphates, color = 'sulphates')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$alcohol, color = 'alcohol')) +
labs(
x = "Lambda",
y = "Coefficients",
title = "The Comparison of the Coefficients w.r.t. Lambda in Rigde"
)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 1, lambda = c(0, 0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.25))
plotData <- data.frame(t(as.matrix(RR$beta)))
ggplot() + theme_bw() +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$fixed.acidity, color = 'fixedAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$volatile.acidity, color = 'volAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$citric.acid, color = 'citricAcid')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$residual.sugar, color = 'resSugar')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$chlorides, color = 'chlorides')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$free.sulfur.dioxide, color = 'freeSO2')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$total.sulfur.dioxide, color = 'totalSO2')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$density, color = 'density')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$sulphates, color = 'sulphates')) +
geom_line(aes(x = seq(0.45,0,-0.05), y = plotData$alcohol, color = 'alcohol')) +
labs(
x = "Lambda",
y = "Coefficients",
title = "The Comparison of the Coefficients w.r.t. Lambda in Lasso"
)
# CV
# Create 5 folds
trainingSet <- trainingSet %>%
mutate(instant = 1:nrow(trainingSet), fold = 0)
tempdata <- trainingSet
for (i in 1:5){
num = 780
if (i == 5){num = 778}
temp <-  sample_n(tempdata, num)
tempdata <- tempdata %>%
filter(!(instant %in% temp$instant))
trainingSet[trainingSet$instant %in% temp$instant,] <- trainingSet %>%
filter(instant %in% temp$instant) %>%
mutate(fold = i)
}
# For each model, for each fold, calculate the best tuning parameter
# First Consider PCR
trainingSet <- trainingSet %>% select(-instant)
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
for (j in 1:10){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
PCR <- pcr(pH ~ .-fold, data = trainingSet2, scale = T)
pred <- test %>% pull(-pH) %*% t(PCR$coefficients[,,j])
MSE[i,j] = mean((pred - test$pH)^2)
}
}
MSE
which.min(apply(MSE,2,mean))
# Find the lowest MSE
# Then Consider PLSR
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
for (j in 1:10){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
PLSR <- plsr(pH ~ .-fold, data = trainingSet2, scale = T)
pred <- test %>% pull(-pH) %*% t(PLSR$coefficients[,,j])
MSE[i,j] = mean((pred - test$pH)^2)
}
}
MSE
which.min(apply(MSE,2,mean))
# Find the lowest MSE
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,0.45,0.05)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,0.45,0.05)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 1, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,0.95,0.09)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,0.9,0.1)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,1.8,0.2)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,90,10)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,27,3)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,18,2)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
which.min(apply(MSE,2,mean))
apply(MSE,2,mean)
# Similarly, Do the same for RR and Lasso
MSE = matrix(rep(0,50), nrow = 5)
for (i in 1:5){
num = 1
for (j in seq(0,9,1)){
trainingSet2 <- trainingSet %>%
filter(fold != i)
test <- trainingSet %>%
filter(fold == i)
test <- test %>%
select(-fold)
RR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = j)
pred <- test %>% pull(-pH) %*% t(RR$beta)
MSE[i,num] = mean((pred - test$pH)^2)
num = num + 1
}
}
MSE
apply(MSE,2,mean)
which.min(apply(MSE,2,mean))
View(trainingSet)
# The best model for PCR is that with 3 PCs,
BestPCR <- pcr(pH ~ .-fold, data = trainingSet, scale = T)
# The best model for PCR is that with 3 PCs,
BestPCR <- pcr(pH ~ .-fold, data = trainingSet, scale = T)
# The best model for PLSR is that with 1 Dimension,
BestPLSR <- plsr(pH ~ .-fold, data = trainingSet, scale = T)
# The best model for Ridge Regression is that with lambda = 3,
BestRR <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 0, lambda = 3)
# The best model for Lasso is that with lambda = 0.2,
BestLasso <- glmnet(as.matrix(trainingSet %>% select(-pH)), as.matrix(trainingSet$pH), alpha = 1, lambda = 0.2)
# Now use the validation set
predPCR <- test %>% pull(-pH) %*% t(PCR$coefficients[,,3])
predPLSR <- test %>% pull(-pH) %*% t(PLSR$coefficients[,,1])
# Now use the validation set
predPCR <- test %>% pull(-pH) %*% t(BestPCR$coefficients[,,3])
predPLSR <- test %>% pull(-pH) %*% t(BestPLSR$coefficients[,,1])
predRR <- test %>% pull(-pH) %*% t(BestRR$beta)
predLasso <- test %>% pull(-pH) %*% t(BestLasso$beta)
# Now use the validation set
predPCR <- validationSet %>% pull(-pH) %*% t(BestPCR$coefficients[,,3])
predPLSR <- validationSet %>% pull(-pH) %*% t(BestPLSR$coefficients[,,1])
predRR <- validationSet %>% pull(-pH) %*% t(BestRR$beta)
predLasso <- validationSet %>% pull(-pH) %*% t(BestLasso$beta)
mean((predPCR - validationSet$pH)^2)
# Now use the validation set
predPCR <- validationSet %>% pull(-pH) %*% t(BestPCR$coefficients[,,3])
predPLSR <- validationSet %>% pull(-pH) %*% t(BestPLSR$coefficients[,,1])
predRR <- validationSet %>% pull(-pH) %*% t(BestRR$beta)
predLasso <- validationSet %>% pull(-pH) %*% t(BestLasso$beta)
mean((predPCR - validationSet$pH)^2)
mean((predPLSR - validationSet$pH)^2)
mean((predRR - validationSet$pH)^2)
mean((predLasso - validationSet$pH)^2)
# Train a model with both training and validation set, and report the performance
semiFinalPCR <- pcr(ph~.-fold, data = bind_rows(trainingSet, validationSet), scale = T)
# Train a model with both training and validation set, and report the performance
semiFinalPCR <- pcr(pH~.-fold, data = bind_rows(trainingSet, validationSet), scale = T)
pred <- bind_rows(trainingSet, validationSet) %>% pull(-pH) %*% t(semiFinalPCR$coefficients[,,3])
mean((testSet$pH - pred)^2)
pred <- testSet %>% pull(-pH) %*% t(semiFinalPCR$coefficients[,,3])
mean((testSet$pH - pred)^2)
pred <- testSet %>% pull(-pH) %*% t(semiFinalPCR$coefficients[,,3])
mean((testSet$pH - pred)^2)
finalPCR <- pcr(pH~.-fold, data = data, scale = T)
finalPCR <- pcr(pH~., data = data, scale = T)
