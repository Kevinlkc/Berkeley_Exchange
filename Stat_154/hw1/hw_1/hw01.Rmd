---
title: "Homework 1 (Coding Part) for Stat 154"
author: "Kevin Luo"
date: "Sept 5, 2019"
output:
  pdf_document: default
  html_document: default
---

\section{Problem 5}
```{r setup, include=FALSE}
library(FactoMineR)
library(ggplot2)
library(tidyverse)
library(factoextra)
data("decathlon")
dat <- decathlon[,1:10]
```

5.1 Should we run PCA on transformed data?  
Yes! Note that here we have different measures of different variables. (Some are measured in time and others in height, etc.) It is probably wise to scale the data first. (i.e. PCA on the basis of correlation matrix)  
Rmk: If the variables are measured on the same scale, it's fair not to scale the data as it's plausible to keep those with more variance que sera sera.
```{r}
# The code shall be:
pca <- PCA(X = dat, scale.unit = TRUE, ncp = 10, graph = FALSE)
```

5.2 Eigenvalues
```{r, fig.height= 3}
# a)
temp <- data.frame(pca$eig[,1])
temp %>%
  mutate(index = 1:10) %>%
  ggplot() +
  theme_bw() +
  geom_col(aes(x = index, y = pca.eig...1.)) +
  labs(
    title = "Eigenvalues",
    x = "index",
    y = "eigenvalues"
  )
```

b) The $i^{th}$ eigenvalue (divided by total dimensions p) is interpreted as the percentage of variance captured by the dimension i.  
c) The projection of originail data on the first dimension of the eigenvector explains 32.7 percent of total inertia. Those on the second dimension explains 17.4 percent of the inertia. Together, they contributes 50.1 percent of the explanation of total variance.  
d) As a rule of thumb, we might arbitrarily choose 1 or 0.7 as a threshold for us to judge whether we retain a Principle Component. In both cases, we should choose to retain the top 4 dimensions with the highest eigenvalues. Collabratively they contributes to almost 75% of the total variation of the original data, and lowers the dimension as well.  

5.3 Interpreting the PCs:
```{r}
# eigenvectors (loadings)
pca$svd$V
# correlations between variables and PCs
pca$var$cor
```

According to the loadings and correlation matrix, we can see that the athletes' performance on the first 7 events (from 100m to 110m hurdle) might be well explained by the first Princical Component, which a correlation mostly >.6 between the variables and the PCs. We might name it jump-advantage/running disadvantage, as the sign in the eigenvectors of running-scores are all negative. (Note that High jump might be an expection, if we arbitrarily set .6 as a threshold. However, the first principal component still explains more of the variation than any other components. So we leave it here.)  
Simimarly, the score of Discus is best explained by the second dimension, Pole vault and 1500m by the third, and Javeline by the fourth.  

5.4 Plot of Individuals:
```{r}
temp <- data.frame(pca$ind$coord[,1:2]) %>%
  mutate(name = rownames(pca$ind$coord[,1:2])) %>%
  mutate(index = 1:41)
temp2 <- temp %>%
  filter(name %in% c("KARPOV","BOURGUIGNON","Casarsa","Lorenzo"))
ggplot() +
  theme_bw() +
  geom_point(data = temp, aes(x = index, y = Dim.1, color = "1st Principal Component")) +
  geom_point(data = temp, aes(x = index, y = Dim.2, color = "2nd Principal Component")) +
  geom_point(data = temp2, aes(x = index, y = Dim.1, color = "1st Principal Component"), size = 5) +
  geom_point(data = temp2, aes(x = index, y = Dim.2, color = "2nd Principal Component"), size = 5) +
  labs(
    x = "index",
    y = "Principal Component", 
    title = "scatterplot of the first two components"
  ) +
  scale_color_manual(values = c("maroon", "pink"))
```

Explanation: The graph above plots all the principal components for athletes in the events. Those expanded points denotes our focused athletes.   
For Karpov in the decathlon, a high positive value of the first dimension indicates a better ability in jumping than running. It's not the case for Bourguignon, Lorenzo and Casarsa, where negative values of in first dimension suggest a strong running ability, but disadvantage in jumping.  
As we discussed before, the second principal component simply denotes the ability of the event Discus. A positive correlation between the PC and the original data shows us that Casarsa performed excellently in this event, and Lorenzo is clearly not good at it. Karpov and Bourguignon are just a little better than average in this particular event.  

\section*{Problem 6}
6.1 PCA-NIPALS algorithm  
```{r}
# starting code
M <- as.matrix(mtcars[ ,c('mpg', 'disp', 'hp', 'wt', 'qsec')])
X <- scale(M)
```

```{r}
# An Overview of the Functions:
## check_convergence
### input: vector a, vector b
### output: boolean, True if a and b are close enough (i.e. converged), False if otherwise

## MyPca
### input: matrix X, with rows indicating the obs., and columns indicating the variables.
### output: environment, containing a matrix for eigenvectors "eigvec", eigenvalues "eigval", and PCs "PC"

check_convergence <- function(a, b){
  flag = TRUE
  for (i in 1:length(a)){
    if (abs(a[i] - b[i]) > 1e-5){
      flag = FALSE
    }
  }
  return(flag)
}
MyPca <- function(X){
  tempX = X
  eig <- environment()
  for (h in 1:ncol(X)) {
    w = rep(1,time = ncol(tempX))
    tempw = rep(-1, time = ncol(tempX))
    while (check_convergence(w, tempw) == FALSE) {
      tempw = w
      w = w / sqrt(sum(w^2))
      z = tempX %*% w / sum(w^2)
      w = t(tempX) %*% z / sum(z^2)
    }
    if (h == 1){
      W = w
      Z = z
    }else{
      W = W %>% append(w)
      Z = Z %>% append(z)
    }
    tempX = tempX - z %*% t(w)
  }
  W = matrix(W, ncol = ncol(tempX))
  Z = matrix(Z, ncol = ncol(tempX))
  assign("PC", Z, envir = eig)
  assign("eigvec", W, envir = eig)
  return(eig)
}
evd <- MyPca(X)
# Note that this should be roughly the same as the following results calculated by bulit-in functions
temp <- PCA(X, scale.unit = TRUE)
evd$eigvec
temp$svd$V
# Another way of confirmation is to verify X = Z %>% t(W) (top 6 rows displayed)
(X - evd$PC %*% t(evd$eigvec)) %>%
  head()
# Hooray!
```

6.2 Calculate the eigenvalues by t(W)SW, where S is the covariance matrix
```{r}
tempX <- scale(X)
t(evd$eigvec) %*% cov(tempX) %*% evd$eigvec
# Note that cov() can also be replaced as \frac{tempX^T tempX}{n}
# The results can also be confirmed by the built-in funciton.
temp$eig[,1]
```

6.3 Two scatterplots of PCs  
First 2 Dimensions:
```{r}
temp2 <- data.frame(evd$PC[,1:2]) %>%
  mutate(index = 1:nrow(evd$PC[,1:2]))
temp2 %>%
ggplot() +
  theme_bw() +
  geom_point(aes(x = index, y = X1, color = "1st Principal Component")) +
  geom_point(aes(x = index, y = X2, color = "2nd Principal Component")) +
  labs(
    x = "index",
    y = "Principal Component", 
    title = "scatterplot of the first two components"
  ) +
  scale_color_manual(values = c("maroon", "gold"))
```
Next 2 Dimensions:
```{r}
temp2 <- data.frame(evd$PC[,3:4]) %>%
  mutate(index = 1:nrow(evd$PC[,3:4]))
temp2 %>%
ggplot() +
  theme_bw() +
  geom_point(aes(x = index, y = X1, color = "3rd Principal Component")) +
  geom_point(aes(x = index, y = X2, color = "4th Principal Component")) +
  labs(
    x = "index",
    y = "Principal Component", 
    title = "scatterplot of the third-fourth components"
  ) +
  scale_color_manual(values = c("maroon", "gold"))
```

Remark: The variation of observasions in the first two dimensions (i.e. Principal Components) are much larger than that in the 
3rd and 4th dimensions.

Plot a Circle of Correlations graph using the first tow dimensions (associated to the first 2 PCs).
```{r}
fviz_pca_var(temp, col.var = "black")
```

The first two principal components are the projection of the orignal data on two orthonogal dimensions that retains the largest inertia. The first dimension in this case explains a huge part of variations in mpg, weight, displacement, and gross hp. It might be considered as an index of car-size. The second principal component, on the other hand, provides an indicaiton of the speed of the car. Together, the two PCs contributes 92.1% of the total variations of the original car-properties data.


