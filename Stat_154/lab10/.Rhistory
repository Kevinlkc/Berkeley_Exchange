install.packages("tree")
library(tree)
install.packages("TREE")
library(tree)
install.packages(tree)
install.packages('devtools') #assuming it is not already installed
library(devtools)
install_github('andreacirilloac/updateR')
install_github('andreacirilloac/updateR')
library(updateR)
library(updateR)
updateR(admin_password = 'kai990630')
install_github('andreacirilloac/updateR')
install.packages(updateR)
install.packages("updateR")
install.packages("tree")
library(tree)
library(MASS)
library(tidyverse)
install.packages("tidyverse")
system('defaults write org.R-project.R force.LANG en_US.UTF-8')
install.packages("tidyr")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("MASS")
library(MASS)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(tree)
install.packages("ISLR")
library(MASS)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(tree)
library(ISLR)
attach(Carseats)
High =ifelse(Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
View(Carseats)
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
plot(tree.carseats)
text(tree.carseats, pretty=0)
train <- sample_frac(Carseats, 0.8)
View(Carseats)
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats, 0.8)
test <- Carseats %>% filter(!(index  %in% train$index))
detach(Carseats)
train <- train[,'-index']
train <- train[,-'index']
train <- train %>% select(-index)
test <- test %>% select(-index)
# Model Fitting
model =tree(High~.-Sales, train)
predict(model, test %>% select(-Sales, -High))
View(test)
test %>% select(-Sales, -High)
?predict
?predict.tree
predict(model, newdata = list(test %>% select(-Sales, -High)))
temp <- test %>% select(-Sales, -High)
predict(model, newdata = list(temp)))
predict(model, newdata = list(temp))
temp <- test %>% select(-Sales, -High)
View(temp)
# Model Fitting
model =tree(High~.-Sales, train)
summary(model)
predict(model, newdata = temp)
temp <- test %>% select(-High)
predict(model, newdata = temp)
predict(model, newdata = temp, type = "class")
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats %>% select(-Sales), 0.8)
test <- Carseats %>% select(-Sales) %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(High~., train)
summary(model)
temp <- test %>% select(-High)
predict(model, newdata = temp, type = "class")
test$High - predict(model, newdata = temp, type = "class")
prediction <- predict(model, newdata = temp, type = "class")
CrossTable(prediction, test$High, chisq = FALSE)
??CrossTable
install.packages("gmodels")
library(gmodels)
library(MASS)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(tree)
library(ISLR)
library(gmodels)
CrossTable(prediction, test$High, chisq = FALSE)
?cv.tree
cv.tree(model, FUN = prune.misclass)
set.seed(1)
cv.tree(model, FUN = prune.misclass)
set.seed(100)
cv.tree(model, FUN = prune.misclass)
# The error is minimized when k = 0.5
?tree
# The error is minimized when k = 0.5
model =tree(High~., train, k=0.5)
# The error is minimized when k = 0.5
model =tree(High~., train, 0.5)
# The error is minimized when k = 0.5
model =tree(High~., train, where = 0.5)
cv.tree(model, FUN = prune.misclass)
set.seed(100)
cv.tree(model, FUN = prune.misclass)
set.seed(1000)
cv.tree(model, FUN = prune.misclass)
set.seed(666)
cv.tree(model, FUN = prune.misclass)
set.seed(142502)
cv.tree(model, FUN = prune.misclass)
set.seed(3)
cv.tree(model, FUN = prune.misclass)
set.seed(142502)
cv.tree(model, FUN = prune.misclass)
# The error is minimized when k = 0.5
prune_model = prune.misclass(tree.carseats , best=22)
plot(prune.carseats )
plot(prune_model )
text(prune_model, pretty=0)
new_pred <- predict(prune_model, test %>% select(-High))
set.seed(142502)
cv.tree(model, FUN = prune.misclass)
# The error is minimized when k = 0.5
prune_model = prune.misclass(model , best=22)
plot(prune_model)
text(prune_model, pretty=0)
new_pred <- predict(prune_model, test %>% select(-High))
CrossTable(new_pred, test$High, chisq = F)
new_pred <- predict(prune_model, test %>% select(-High))
predict(prune_model, test %>% select(-High))
new_pred <- predict(prune_model, test %>% select(-High), type = "class")
CrossTable(new_pred, test$High, chisq = F)
library(MASS)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(tree)
library(ISLR)
library(gmodels)
attach(Carseats)
High =ifelse(Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
detach(Carseats)
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats %>% select(-Sales), 0.8)
test <- Carseats %>% select(-Sales) %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(High~., train)
summary(model)
temp <- test %>% select(-High)
prediction <- predict(model, newdata = temp, type = "class")
CrossTable(prediction, test$High, chisq = FALSE)
# The accuracy is 61/80
set.seed(142502)
cv.tree(model, FUN = prune.misclass)
# The error is minimized when k = 0.5
prune_model = prune.misclass(model , best=9)
plot(prune_model)
text(prune_model, pretty=0)
new_pred <- predict(prune_model, test %>% select(-High), type = "class")
CrossTable(new_pred, test$High, chisq = F)
# The accuracy is still 61/80, though predictions do change.
attach(Carseats)
High =ifelse(Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
tree.carseats =tree(High~.-Sales, Carseats)
High =ifelse(Carseats$Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
High =ifelse(Carseats$Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
tree.carseats =tree(High~.-Sales, Carseats)
library(MASS)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(tree)
library(ISLR)
library(gmodels)
High =ifelse(Carseats$Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
library(MASS)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(tree)
library(ISLR)
library(gmodels)
High =ifelse(Carseats$Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats %>% select(-Sales), 0.8)
test <- Carseats %>% select(-Sales) %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(High~., train)
summary(model)
temp <- test %>% select(-High)
prediction <- predict(model, newdata = temp, type = "class")
CrossTable(prediction, test$High, chisq = FALSE)
# The accuracy is 61/80
set.seed(142502)
cv.tree(model, FUN = prune.misclass)
# The error is minimized when k = 0.5
prune_model = prune.misclass(model , best=22)
plot(prune_model)
text(prune_model, pretty=0)
new_pred <- predict(prune_model, test %>% select(-High), type = "class")
CrossTable(new_pred, test$High, chisq = F)
# The accuracy is still 61/80, though predictions do change.
library(MASS)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(tree)
library(ISLR)
library(gmodels)
High =ifelse(Carseats$Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats %>% select(-Sales), 0.8)
test <- Carseats %>% select(-Sales) %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(High~., train)
summary(model)
temp <- test %>% select(-High)
prediction <- predict(model, newdata = temp, type = "class")
CrossTable(prediction, test$High, chisq = FALSE)
# The accuracy is 61/80
set.seed(142502)
cv.tree(model, FUN = prune.misclass)
# The error is minimized when k = 0.5
prune_model = prune.misclass(model , best=22)
plot(prune_model)
text(prune_model, pretty=0)
new_pred <- predict(prune_model, test %>% select(-High), type = "class")
CrossTable(new_pred, test$High, chisq = F)
# The accuracy is still 61/80, though predictions do change.
library(MASS)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
library(tree)
library(ISLR)
library(gmodels)
Carseats <- Carseats %>% mutate(High =ifelse(Carseats$Sales<=8, "No", "Yes"))
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
Carseats <- Carseats %>% mutate(High =ifelse(Carseats$Sales<=8, "No", "Yes"))
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
View(Carseats)
Carseats <- Carseats %>% mutate(High =ifelse(Carseats$Sales<=8, "No", "Yes")) %>% select(-Sales)
tree.carseats =tree(High~.-Sales, Carseats)
tree.carseats =tree(High~., Carseats)
View(tree.carseats)
Carseats <- Carseats %>% mutate(High =ifelse(Carseats$Sales<=8, "No", "Yes")) %>% select(-Sales)
Carseats
Carseats <- Carseats %>% mutate(High =ifelse(Carseats$Sales<=8, "No", "Yes")) %>% select(-Sales)
Carseats <- Carseats %>% mutate(High =ifelse(Carseats$Sales<=8, "No", "Yes"))
data("Carseats")
Carseats <- Carseats %>% mutate(High =ifelse(Carseats$Sales<=8, "No", "Yes"))
Carseats <- Carseats %>% select(-Sales)
tree.carseats =tree(High~., Carseats)
tree.carseats <- tree(Carseats$High ~ Carseats %>% select(-High))
tree.carseats <- tree(Carseats$High ~. )
tree.carseats <- tree(Carseats$High ~. , data = Carseats)
High =ifelse(Carseats$Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
data("Carseats")
High =ifelse(Carseats$Sales<=8, "No", "Yes")
Carseats =data.frame(Carseats, High)
tree.carseats =tree(High~.-Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats %>% select(-Sales), 0.8)
test <- Carseats %>% select(-Sales) %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(High~., train)
summary(model)
temp <- test %>% select(-High)
prediction <- predict(model, newdata = temp, type = "class")
CrossTable(prediction, test$High, chisq = FALSE)
# The accuracy is 61/80
set.seed(1)
set.seed(1)
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats %>% select(-Sales), 0.8)
test <- Carseats %>% select(-Sales) %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(High~., train)
summary(model)
temp <- test %>% select(-High)
prediction <- predict(model, newdata = temp, type = "class")
CrossTable(prediction, test$High, chisq = FALSE)
# The accuracy is 61/80
set.seed(100)
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats %>% select(-Sales), 0.8)
test <- Carseats %>% select(-Sales) %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(High~., train)
summary(model)
temp <- test %>% select(-High)
prediction <- predict(model, newdata = temp, type = "class")
CrossTable(prediction, test$High, chisq = FALSE)
# The accuracy is 61/80
set.seed(142502)
# Non-random splitting of dataset
train <- Carseats[1:320,]
test <- Carseats[321:400,]
# Or random splitting of dataset
Carseats <- Carseats %>% mutate(index = 1:400)
train <- sample_frac(Carseats %>% select(-Sales), 0.8)
test <- Carseats %>% select(-Sales) %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(High~., train)
summary(model)
temp <- test %>% select(-High)
prediction <- predict(model, newdata = temp, type = "class")
CrossTable(prediction, test$High, chisq = FALSE)
# The accuracy is 61/80
set.seed(142502)
cv.tree(model, FUN = prune.misclass)
# The error is minimized when k = 0.5
prune_model = prune.misclass(model , best=22)
plot(prune_model)
text(prune_model, pretty=0)
set.seed(142502)
cv.tree(model, FUN = prune.misclass)
# The error is minimized when k = 4
prune_model = prune.misclass(model , best=8)
plot(prune_model)
text(prune_model, pretty=0)
new_pred <- predict(prune_model, test %>% select(-High), type = "class")
CrossTable(new_pred, test$High, chisq = F)
# The accuracy is still 61/80, though predictions do change.
data(Boston)
tree.boston =tree(medv~., Boston)
summary(tree.carseats)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty=0)
set.seed(142502)
# random splitting of dataset
Boston <- Boston %>% mutate(index = 1:400)
set.seed(142502)
# random splitting of dataset
Boston <- Boston %>% mutate(index = 1:506)
train <- sample_frac(Boston, 0.8)
test <- Boston %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# We'll use the latter
# Model Fitting
model =tree(mdev~., train)
# Model Fitting
model =tree(medv~., train)
summary(model)
prediction <- predict(model, test %>% select(-medv), type = "class")
prediction <- predict(model, test %>% select(-medv))
MSE <- mean((prediction - test$medv)^2)
?cv.tree
cv.tree(model)
set.seed(100)
cv.tree(model)
# The error is minimized when k = 4
prune_model = prune.misclass(model , best=8)
set.seed(100)
# random splitting of dataset
Boston <- Boston %>% mutate(index = 1:506)
train <- sample_frac(Boston, 0.8)
test <- Boston %>% filter(!(index  %in% train$index))
train <- train %>% select(-index)
test <- test %>% select(-index)
# Model Fitting
model =tree(medv~., train)
summary(model)
prediction <- predict(model, test %>% select(-medv))
# The MSE of the original model is 22.59
mean((prediction - test$medv)^2)
set.seed(100)
cv.tree(model)
cv.tree(model)
# The error is minimized when k = -Inf, So we still going to stick to the original model
prune_model = prune.tree(model)
plot(prune_model)
text(prune_model, pretty=0)
prediction <- predict(prune_model, test %>% select(-medv))
cv.tree(model)
# The error is minimized when k = -Inf, So we still going to stick to the original model, without pruning.
# So we can repeat the above process
model =tree(medv~., train)
summary(model)
prediction <- predict(model, test %>% select(-medv))
# The MSE of the original model is 35.0
mean((prediction - test$medv)^2)
